# -*- coding: utf-8 -*-
"""Jaume-generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kq8gpsBES_DqLmRA6ZnAsyHEPs5sTiG4
"""

import torch
import torch.nn as nn
import torch
def vocabulary(words):
  """
  Given a plain text in a list returns a list with all the different words.
  """
  vocab = set()
  for word in words:
    if word not in vocab:
      vocab.add(word)

  return list(vocab)

#Decide what to do with words that are not in the vocabulary
def text_to_seq(text,vocab,add_n = False):
  """
  Given a text split it and a vocabulary, return a list of numbers that refers 
  to the index position of each word in the vocabulary.
  """
  sequence = []
  for word in text:
    if word != "":
      sequence.append(vocab.index(word))

  return torch.tensor(sequence)

def data_process(corpus, vocab):
    data = list()
    for text in corpus:
        token_list = text_to_seq(text.split(" "),vocab,False)
        for i in range(1, len(token_list)):
            n_gram_seq = torch.tensor(token_list[:i+1], dtype=torch.long)
            data.append(n_gram_seq)
    return data

with open("llibres_dataset_v3.txt","r") as f:
  dataset = ""
  for line in f:
    dataset += line

for_vocabulary = dataset.replace("\n"," ")
sentences = dataset.split("\n")
vocab = vocabulary(for_vocabulary.split(" "))
print("Number of words",len(for_vocabulary.split(" ")))
print("Number of different words",len(vocab))
print("Number of sentences",len(sentences))

train_data = data_process(sentences, vocab)

X = [i[:-1] for i in train_data]   # taking all the words except the last in the input set
y = [i[-1] for i in train_data]    # taking last words in the output set
X_lite = X
y_lite = y
print("X shape",len(X))
print("y shape",len(y))

class LSTM_model(nn.Module):
    def __init__(self, input_size, hidden_size, output_size,dropout):
        super(LSTM_model, self).__init__()
        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)
        self.lstm1 = nn.LSTM(input_size, hidden_size)
        self.lstm2 = nn.LSTM(hidden_size, hidden_size)
        self.fc = nn.Linear(hidden_size, output_size)
        #self.sigm = nn.Sigmoid()
        #self.dropout = nn.Dropout(dropout)

    def zero_hidden(self):
        # Initialize lstm states with zeros
        return (torch.zeros(1, 1, hidden_dim).cuda(),
                torch.zeros(1, 1, hidden_dim).cuda())

    def forward(self, input):
        x = self.embed(input).unsqueeze(1) #We need the unsqueze to introduce the new dimension
        output_lstm, states_lstm = self.lstm1(x, self.zero_hidden())
        output_lstm, states_lstm = self.lstm2(output_lstm, self.zero_hidden())
        #output_lstm = self.dropout(output_lstm)
        output_fc = self.fc(output_lstm)
        #out = self.sigm(output_fc)
        return output_fc, output_lstm, states_lstm

num_epochs = 10
embedding_dim = 400
hidden_dim = 600
vocab_size = len(vocab)

LSTM_model = LSTM_model(embedding_dim, hidden_dim, vocab_size,dropout=None).cuda()

CE_loss = torch.nn.CrossEntropyLoss() 
#loss = nn.NLLLoss()

optimizer_LSTM = torch.optim.Adam(list(LSTM_model.parameters()), lr=0.000005,weight_decay=0.001)

def train_loop(model, num_epochs, CE_loss, optimizer, X,y):

    loss_hist = []

    for x in range(num_epochs):
        print('Epoch: {}'.format(x))
        optimizer.zero_grad()
        # TRAINING LOOP
        for i,instance in enumerate(X):
            val = instance.cuda()
            #y_aux = np.zeros(len(vocab))
            #y_aux[int(y[i])] = 1.0
            y_val = y[i].cuda()
            #y_val = torch.tensor(y_aux).cuda()
            y_hat, _, _ = model(val)
            # Calculate loss
            loss = CE_loss(y_hat[0], y_val.reshape((-1,)))
            # Backpropagate
            loss.backward()
            #Clip gradient
            nn.utils.clip_grad_norm_(model.parameters(), 5)
            # Update weights
            optimizer.step()

        print('Loss: {:6.4f}'.format(loss.item()))
        loss_hist.append(loss.item())
    return loss_hist

loss_hist = train_loop(LSTM_model, num_epochs, CE_loss, optimizer_LSTM, X, y)

torch.save(LSTM_model.state_dict(),'language_generator-600.ckpt')

def tensor_to_str(tensor,vocab):
  text = ""
  for elem in tensor:
    text += " "+ vocab[int(elem)]
  return text

sig = nn.Sigmoid()

def generator(model,n_words,vocab,seed):
  result = text_to_seq(seed.split(" "),vocab).cuda()
  final_list = list(result)
  for n in range(n_words):
    #Instead of max sample
    #print(torch.tensor(final_list))
    val = model(torch.tensor(final_list).cuda())[0][0]
    val = sig(val)
    final_list.append(val.multinomial(num_samples=1))
  return tensor_to_str(final_list,vocab)
seed = "bon dia"
print(generator(LSTM_model,5,vocab,seed))

#loss 6.6
#bon dia lluminosa bohemis criatura penediment nord